<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>f7b01be818c545af9375b55ec94a85e1</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="csci-470-activities-and-case-studies" class="cell markdown"
id="C5gi_BKO2wNi">
<h2>CSCI 470 Activities and Case Studies</h2>
<ol>
<li>For all activities, you are allowed to collaborate with a
partner.</li>
<li>For case studies, you should work individually and are
<strong>not</strong> allowed to collaborate.</li>
</ol>
<p>By filling out this notebook and submitting it, you acknowledge that
you are aware of the above policies and are agreeing to comply with
them.</p>
</section>
<div class="cell markdown" id="-nz2fmXy2wNk">
<p>Some considerations with regard to how these notebooks will be
graded:</p>
<ol>
<li>Cells in which "# YOUR CODE HERE" is found are the cells where your
graded code should be written.</li>
<li>In order to test out or debug your code you may also create notebook
cells or edit existing notebook cells other than "# YOUR CODE HERE". We
actually highly recommend you do so to gain a better understanding of
what is happening. However, during grading, <strong>these changes are
ignored</strong>.</li>
<li>You must ensure that all your code for the particular task is
available in the cells that say "# YOUR CODE HERE"</li>
<li>Every cell that says "# YOUR CODE HERE" is followed by a "raise
NotImplementedError". You need to remove that line. During grading, if
an error occurs then you will not receive points for your work in that
section.</li>
<li>If your code passes the "assert" statements, then no output will
result. If your code fails the "assert" statements, you will get an
"AssertionError". Getting an assertion error means you will not receive
points for that particular task.</li>
<li>If you edit the "assert" statements to make your code pass, they
will still fail when they are graded since the "assert" statements will
revert to the original. Make sure you don't edit the assert
statements.</li>
<li>We may sometimes have "hidden" tests for grading. This means that
passing the visible "assert" statements is not sufficient. The "assert"
statements are there as a guide but you need to make sure you understand
what you're required to do and ensure that you are doing it correctly.
Passing the visible tests is necessary but not sufficient to get the
grade for that cell.</li>
<li>When you are asked to define a function, make sure you
<strong>don't</strong> use any variables outside of the parameters
passed to the function. You can think of the parameters being passed to
the function as a hint. Make sure you're using all of those
variables.</li>
<li>Finally, <strong>make sure you run "Kernel &gt; Restart and Run
All"</strong> and pass all the asserts before submitting. If you don't
restart the kernel, there may be some code that you ran and deleted that
is still being used and that was why your asserts were passing.</li>
</ol>
</div>
<section id="case-study-bayesian-classifier" class="cell markdown"
id="x0GekQLg2wNl">
<h1>Case Study: Bayesian classifier</h1>
<p>In this two-part case study you will:</p>
<ol>
<li>Build, train, and test a Bayesian classifier. This will increase
your understanding of parametric models for supervised learning.</li>
<li>Implement a Bayesian classifier model as Python class, in a manner
similar to those of scikit-learn. This will improve you understanding of
what goes on under the hood of scikit-learn models.</li>
<li>Evaluate performance of your Bayesian classifer.</li>
<li>Repeat this process, but build a Foolish classifer rather than a
Bayesian classifier, then compare performance of a Foolish classifier to
a Bayesian classifier. This will alert you to the sometimes misleadingly
high performance scores a model might give, even when it has not learned
any relationship between features and targets.</li>
</ol>
<h2
id="part-2---comparing-a-foolish-classifier-to-a-bayesian-classifier">Part
2 - Comparing a "Foolish" classifier to a Bayesian classifier</h2>
<h3
id="in-this-notebook-you-will-build-a-foolish-classifier--one-which-only-learns-from-the-targetslabels-but-not-the-from-features-youll-then-compare-evaluation-results-with-those-of-a-scikit-learn-naive-bayesian-classifier">In
this notebook you will build a foolish classifier--one which only learns
from the targets/labels, but not the from features. You'll then compare
evaluation results with those of a scikit-learn Naive Bayesian
classifier.</h3>
</section>
<section id="grading" class="cell markdown" id="nTGiwDVH2wNm">
<h3>Grading</h3>
<p>50 points total</p>
<ol>
<li>10 pts - <code>fit</code> method</li>
<li>10 pts - <code>predict</code> and <code>fit_predict</code>
methods</li>
<li>5 pts - Training models and making test set predictions for 4
classifiers</li>
<li>5 pts - <code>my_accuracy_score</code> function</li>
<li>15 pts - <code>my_confusion_matrix</code> function</li>
<li>5 pts - Using scikit-learn to get F-scores</li>
</ol>
</section>
<section id="the-gaussian-naive-bayes-classifer" class="cell markdown"
id="44a4cTyi2wNm">
<h2>The Gaussian Naive Bayes classifer</h2>
<p>In Part 1 of this study, you created a Bayes classifier that modeled
the distributions of features as 2D Gaussian distributions, but ones
constrained to having a diagonal covariance matrix--that is, Gaussian
shapes like in the image below, having no "diagonal" aspect to the
elliptical shape of the distribution.</p>
<table>
<colgroup>
<col style="width: 100%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><img src="./2d_noisy_gaussian.png"
alt="A 2D Gaussian distribution with diagonal covariance matrix, fitted to noisy data." /></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">A 2D Gaussian distribution with diagonal
covariance matrix, fitted to noisy data.</td>
</tr>
</tbody>
</table>
<p>Hopefully you learned a lot by taking on that challenging task! In
this notebook you'll train a much dumber, Foolish Classifier, and while
it'd be nice to compare its results to those of your Bayes Classifier,
we want to be 100% certain that the Bayes Classifier is implemented
without error, so we'll use scikit-learn's Gaussian Naive Bayes
classifier instead. Performance of the Naive Bayes classifier (for the
feature distributions we're assuming and using) is nearly as good as
that of our Bayes classifier.</p>
<p>Recall that a Naive Bayes classifier assumes that the individual
features of our multivariate feature distribution are independent.</p>
<p><span
class="math display"><em>P</em>(<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>|<em>y</em>) = <em>P</em>(<em>x</em><sub>1</sub>|<em>y</em>)<em>P</em>(<em>x</em><sub>2</sub>|<em>y</em>)</span></p>
<p>In Part 1, our <span
class="math inline"><em>P</em>(<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>|<em>y</em>)</span>
was:</p>
<p><span class="math display">$$ P(x_1,x_2|y)=\frac{
exp\Big(-(\frac{{(x_1-\mu_{y,1})}^2}{\sigma_{y,1}^2} +
\frac{{(x_2-\mu_{y,2})}^2}{\sigma_{y,2}^2}) /2\Big)
}{2\pi\sigma_{y,1}\sigma_{y,2}}  $$</span></p>
<p>But under the Naive Bayes (false) independence assumption, the
probability equation becomes:</p>
<p><span class="math display">$$ P(x_1,x_2|y)=P(x_1|y)P(x_2|y)=\frac{
exp\Big(-\frac{{(x_1-\mu_{y,1})}^2}{\sigma_{y,1}^2}\Big)exp\Big(-\frac{{(x_2-\mu_{y,2})}^2}{\sigma_{y,2}^2}\Big)}{2\pi\sigma_{y,1}\sigma_{y,2}}
$$</span></p>
<p>If we were to plot this distribution, you'd see one that has a
slightly more rectangular shape, when compared to the elliptical shape
of the figure above.</p>
<p>The feature independence assumption makes it much easier to estimate
the parameters of the probability distribution even when there are many
features and limited data. After your experience in Part 1, you could
likely implement the naive distribution quite easily, with only minor
changes to your BayesClassifier code, <strong>but we'll use the
scikit-learn Gaussian Naive Bayes classifier to ensure that it's
correct</strong>.</p>
</section>
<section id="the-foolish-classifier" class="cell markdown"
id="DUB9NDvz2wNn">
<h2>The Foolish classifier</h2>
<p>In this notebook you'll implement a foolish classifier. It's foolish
because although the user passes both features and labels (targets) to
it for training, the classifier completely ignores the features. In
effect, it only estimates the prior distribution of the classes, <span
class="math inline"><em>P</em>(<em>y</em>)</span>, and even that it
sometimes does poorly.</p>
<p><strong>Why are we having you do this?</strong> As you'll see from
the evaluation scores of the Foolish classifier and the Naive Bayes
classifier, some evaluation metrics can give surprisingly high scores
for such a foolish approach. If you use just one or two evaluation
metrics alone, you might be deceived into thinking that the foolish
classifier is quite wise! But in fact, you'll know that it's really just
"guessing" as to what the true labels are, not predicting based upon
informative features. If you were to employ such a classifier, it'd
eventually be discovered and hopefully you'd be fired. :)</p>
<p>You'll implement a foolish classifier that has <strong>three levels
of foolishness</strong>, one of which can be selected by the user when
instantiating the class object:</p>
<ol>
<li><strong>Uniform</strong>: P(y) is a uniform distribution. That is,
aside from knowing the number of classes, it doesn't even learn from the
targets themselves. It just assumes that all classes are equally
likely.</li>
<li><strong>Mode</strong>: P(y) has probability of 1 for the class that
occurs most often in the training labels, and 0 for all other classes.
It predicts the "mode" of the training labels--the prediction never
changes, it is always the same class.</li>
<li><strong>Proportional</strong>: P(y) probabilities are proportional
to the frequency of labels in the training data. This is exactly how you
were instructed to estimate the priors of the Bayes Classifier of Part
1. For example, if classes 0, 1, and 2, have 5, 10, and 35 samples,
respectively, then the P(y) foolish classifier's priors for those
classes will be 5/50, 10/50, and 35/50 (0.1, 0.2, and 0.7).</li>
</ol>
<p>They're all foolish, but <strong>which do you think will score most
highly</strong> on evaluation metrics applied to the test set? Or to the
training set, for that matter, since the classifier doesn't even use the
features of the training set for any learning. <strong>Ponder this for a
moment before you dive in!</strong></p>
</section>
<section id="the-models-python-class" class="cell markdown"
id="1PShGXM-2wNn">
<h2>The model's Python class</h2>
<p>As in Part 1, you will create a Python class from scratch (aside from
the skeleton we have provided) that will implement three methods that
are part of most scikit-learn model classes:</p>
<p><code>fit(X, y)</code> - Trains the model using features, X, and
labels, y, and stores the learned (fitted) model parameters as class
object attributes. <strong>Except in this case the foolish classifier
will take in feature and labels, but then completely ignore the
features.</strong></p>
<p><code>predict(X)</code> - Using the stored model parameters, predicts
labels for features, X, and returns them. <strong>But again, in this
case, those predictions will not utilize the information of the features
at all. The model will effectively be guessing.</strong></p>
<p><code>fit_predict(X, y)</code> - Sequentially performs the actions of
<code>fit()</code> and <code>predict()</code>.</p>
<p>Refer back to Part 1 if you need to. Here, we'll just instruct you on
aspects that are specific to what we'll name the
<code>FoolishClassifier</code> class. In addition to the
<code>n_classes</code> argument that your <code>BayesClassifier</code>'s
<code>__init__</code>method took, the <code>FoolishClassifier</code>
will also need a <code>prior_distribution</code> argument. The
<code>prior_distribution</code> argument should be one of three
strings:</p>
<ul>
<li><strong><code>'uniform'</code></strong>: Your <code>fit</code>
method literally does nothing. When your model makes predictions, it
just makes a completely random guess, with equal probability for each
class.</li>
<li><strong><code>'mode'</code></strong>: Your <code>fit</code> method
will determine the most frequent class of the labels, store it, and all
its predictions will be of that class.</li>
<li><strong><code>'proportional'</code></strong>: Your <code>fit</code>
method's estimates of class prior probabilities are simply the
fractional frequency of class occurances of the <code>labels</code> it
is given, just as in Part 1 of the case study. For <code>predict</code>
your model will randomly draw/sample from a distribution that has those
priors (we'll guide you on how to do that).</li>
</ul>
</section>
<div class="cell code" id="Ayoq95PY2wNo">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.naive_bayes <span class="im">import</span> GaussianNB</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_recall_fscore_support</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">&quot;ggplot&quot;</span>)</span></code></pre></div>
</div>
<section id="foolishclassifierfit-method" class="cell markdown"
id="19YkLwEX2wNp">
<h2>FoolishClassifier.fit method</h2>
<p>You are welcome to implement all the methods at once. However, we've
created test cells and visualizations and/or printouts that will guide
you first through the testing of your <code>fit</code> method, then
followed by ones for your <code>predict</code> method. So you may want
to first work solely on <code>fit</code>, until you pass the test cell
that follows. Then work on <code>predict</code> until is passes the
subsequent test cell.</p>
<p>Guidance on implementing <code>fit</code> is in the comments in the
skeleton class below. Go for it!</p>
<p><strong>You should use <code>numpy.random.choice()</code> to help
with your <code>predict</code> method</strong>. See guidance in a cell
further below for more info, before coding your <code>predict</code>. Do
not use other methods, e.g., <code>random.uniform()</code> or
<code>numpy.random.uniform</code>, for making random predictions. Other
methods would be fine, generally, but you'll need to use
<code>choice</code> in order to pass the assert statements.</p>
</section>
<div class="cell code" data-deletable="false" id="ybI_Sk-v2wNp"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;b38107b57420b488339d32af950d1a76&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;ClassDefinition&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">## Implement the three methods of your Foolish classifier.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FoolishClassifier():</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, prior_distribution, n_classes):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Store the type of prior distribtion and number of</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># classes as class attributes.</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> prior_distribution <span class="kw">in</span> [<span class="st">&#39;uniform&#39;</span>, <span class="st">&#39;mode&#39;</span>, <span class="st">&#39;proportional&#39;</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior_distribution <span class="op">=</span> prior_distribution</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_classes <span class="op">=</span> n_classes</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># You may initialize other variables here if you want to,</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but it&#39;s not truly necessary. You can do that in .fit().</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> prior_distribution <span class="op">==</span> <span class="st">&#39;mode&#39;</span>:</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mode <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> prior_distribution <span class="op">==</span> <span class="st">&#39;proportional&#39;</span>:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.priors <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># raise NotImplementedError()</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, features, labels):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># features - a numpy array of shape (n_samples, n_features)</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels - a numpy array of shape (n_samples,)</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This method does not return/output anything.</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use &#39;labels&#39; to fit your model parameters. Ignore &#39;features&#39; altogether!</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If &#39;prior_distribution&#39; is:</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. &#39;uniform&#39; - Do nothing. There is no information in &#39;labels&#39; that you need for predictions.</span></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                Just exit the function via a &#39;pass&#39; or empty &#39;return&#39; statement.</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. &#39;mode&#39; - Determine which label/class occurs most frequently.</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#             ** Save that label, the mode, as &quot;self.mode&quot;.</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. &#39;proportional&#39; - For each class, count the number of occurrances of</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                     that class and divide by the total number of samples</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                     to get the estimated prior for that class.</span></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                     ** Save the priors in a list or numpy array named &quot;self.priors&quot;,</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                        ordering them by their class label (the prior for</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                        class 0 is first, the prior for n_classes-1 is last).</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># As a reminder, if you use the range() function to loop over</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the class labels [0, 1, 2, ...] recall that range() excludes the</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># value of the argument you give it. That is, range(4) will iterate</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># over 0, 1, 2, and 3. It will exclude 4.</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.prior_distribution <span class="op">==</span> <span class="st">&#39;mode&#39;</span>:</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mode <span class="op">=</span> np.bincount(labels).argmax()</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.prior_distribution <span class="op">==</span> <span class="st">&#39;proportional&#39;</span>:</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.priors <span class="op">=</span> [np.mean(labels <span class="op">==</span> i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_classes)]</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># raise NotImplementedError()</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, features):</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># features - a numpy array of shape (n_samples, n_features)</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns an numpy array of predictions</span></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Because the foolish classifier does not use features to make</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># predictions, the only piece of information you need to extract</span></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># from &#39;features&#39; is the number of samples passed in, which is</span></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the number predictions that need to be made and returned.</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If &#39;prior_distribution&#39; is:</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. &#39;uniform&#39; - Draw predictions from a random distribution and</span></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                return them. You may use numpy.random.choice()</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                to help you.</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. &#39;mode&#39; - Return an array of length equal to that of</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>        <span class="co">#             the number of samples in &#39;features&#39;, with each</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>        <span class="co">#             element being your stored mode value, &#39;self.mode&#39;.</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. &#39;proportional&#39; - Draw predictions from a probability disribution</span></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                     with probabilities equal to those of your</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                     stored &#39;self.priors&#39;. You may use numpy.random.choice()</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>        <span class="co">#                     to help you.</span></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> features.shape[<span class="dv">0</span>]</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.prior_distribution <span class="op">==</span> <span class="st">&#39;uniform&#39;</span>:</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> np.random.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="va">self</span>.n_classes, size<span class="op">=</span>n_samples)</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.prior_distribution <span class="op">==</span> <span class="st">&#39;mode&#39;</span>:</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> np.full((n_samples,), <span class="va">self</span>.mode)</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="va">self</span>.prior_distribution <span class="op">==</span> <span class="st">&#39;proportional&#39;</span>:</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> np.random.choice(np.arange(<span class="va">self</span>.n_classes), size<span class="op">=</span>n_samples, p<span class="op">=</span><span class="va">self</span>.priors)</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> predictions</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>        <span class="co"># raise NotImplementedError()</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit_predict(<span class="va">self</span>, features, labels):</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># features - a numpy array of shape (n_samples, n_features==2)</span></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>        <span class="co"># labels - a numpy array of shape (n_samples,)</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>        <span class="co">#</span></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Returns an array of predictions</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Fit your model by calling self.fit(features, labels)</span></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Then make predictions by calling self.predict(features)</span></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Finally, return those predictions.</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># YOUR CODE HERE</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fit(features, labels)</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> <span class="va">self</span>.predict(features)</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> predictions</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell markdown" id="sDimeyWt2wNq">
<p>The cell below does a coarse check of your implementation of the
FoolishClassifier's fit <code>method</code>. It may not give you much
meaningful feedback to help you debug your code.</p>
<p>To help with debugging, don't hesistate to:</p>
<ol>
<li>Put print statements in your code above, to show you the contents of
variables and ensure they hold the values you expect. Also print out
helpful info such as a variables type (e.g., are you sure it's a numpy
array? Or is it a list?... <code>print(type(my_variable))</code>, shape:
<code>print(my_variable.shape)</code>, and length:
<code>print(len(my_variable))</code>. Note that the output of the print
statements will appear below the cell in which your classifier is
instantiated, or in which one of its methods is called. It will not
appear under the cell in which the class is defined (above).</li>
<li>Create a new cell below, and create simple data you can use to test
your classifier (as in the autograder cell below).</li>
<li>If you instantiate your class, you can then view its attributes and
their values as shown below:</li>
</ol>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create object</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FoolishClassifier(<span class="st">&#39;proportional&#39;</span>, n_classes<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># print out the names of its attributes</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf.__dict__.keys())</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># If an attribute is named &quot;self.priors&quot; you can print out its values...</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf.priors)</span></code></pre></div>
<p>Keep in mind that depending on how you implemented your class, some
attributes may not exist until the <code>fit()</code> method is
called.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-deletable="false" data-editable="false" id="1T2_KX9o2wNr"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;35a7f0a1fbe56bcba13c25c88f1979c2&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;Test_fit&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}"
data-outputId="0a1e6ba0-b9b6-4fc2-da71-3185e399c978">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Autograder: Test FoolishClassifier.fit()</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>n_classes <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.choice(n_classes, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Test &#39;uniform&#39;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FoolishClassifier(<span class="st">&#39;uniform&#39;</span>, n_classes)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Test &#39;mode&#39;</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FoolishClassifier(<span class="st">&#39;mode&#39;</span>, n_classes)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf.mode)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> clf.mode<span class="op">==</span><span class="dv">3</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Test &#39;proportional&#39;</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FoolishClassifier(<span class="st">&#39;proportional&#39;</span>, n_classes)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clf.priors)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.<span class="bu">all</span>(np.isclose(clf.priors, [<span class="fl">0.258</span>, <span class="fl">0.23</span>, <span class="fl">0.232</span>, <span class="fl">0.28</span>]))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>3
[0.258, 0.23, 0.232, 0.28]
</code></pre>
</div>
</div>
<section id="foolishclassifierpredict-and-fit_predict-methods"
class="cell markdown" id="5uMLfG7D2wNr">
<h2>FoolishClassifier.predict and fit_predict methods</h2>
<p>If you didn't implement <code>predict</code> and
<code>fit_predict</code> already, now is the time to do so. The methods
will be tested in the test cell below. Guidance is provided in the
comments in the class skeleton.</p>
<p><strong>You should use <code>numpy.random.choice()</code> to help
with your <code>predict</code> method</strong>. The function draws
sample from a discrete probability function that you define. Read the <a
href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html">numpy.random.choice
documentation</a> carefully. Because we want each sample to be
independently chosen/sampled/drawn, what is the correct setting for the
<code>replace</code> argument?</p>
<p>For those with greater math and programming skills, you may want to
challenge yourself... Think about how you would implement
<code>predict</code> if <code>numpy.random.choice</code> was not
available, and you only had a uniform random number generator (random
floating-point numbers between 0 and 1) available to you. (Don't
actually use such an implementation, however, since you will almost
certainly fail the assert statements if you do, since they rely on the
known results from <code>numpy.random</code>, with the seed it is
given.)</p>
</section>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-deletable="false" data-editable="false" id="giLd3Uix2wNs"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;5ef5d8f0f0cf650278877dc272a2cd74&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;Test_predict&quot;,&quot;locked&quot;:true,&quot;points&quot;:10,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}"
data-outputId="33bbf102-f279-47e1-9d92-0584ca20d882">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Autograder: Test FoolishClassifier.predict() and fit_predict()</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">42</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>n_classes <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>]<span class="op">*</span><span class="dv">100</span> <span class="op">+</span> [<span class="dv">1</span>]<span class="op">*</span><span class="dv">200</span> <span class="op">+</span> [<span class="dv">2</span>]<span class="op">*</span><span class="dv">300</span> <span class="op">+</span> [<span class="dv">3</span>]<span class="op">*</span><span class="dv">400</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>np.random.shuffle(y)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.zeros((<span class="dv">1000</span>, <span class="dv">10</span>))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Test &#39;uniform&#39;</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FoolishClassifier(<span class="st">&#39;uniform&#39;</span>, n_classes)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> clf.predict(X)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>label, counts <span class="op">=</span> np.unique(predictions, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(counts)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.array_equal(counts, [<span class="dv">267</span>, <span class="dv">248</span>, <span class="dv">257</span>, <span class="dv">228</span>])</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Test &#39;mode&#39;</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FoolishClassifier(<span class="st">&#39;mode&#39;</span>, n_classes)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> clf.predict(X)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>label, counts <span class="op">=</span> np.unique(predictions, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> label[<span class="dv">0</span>]<span class="op">==</span>clf.mode</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(counts)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.array_equal(counts, [<span class="dv">1000</span>])</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Test &#39;proportional&#39;</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FoolishClassifier(<span class="st">&#39;proportional&#39;</span>, n_classes)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> clf.predict(X)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>label, counts <span class="op">=</span> np.unique(predictions, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(counts)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.array_equal(counts, [<span class="dv">103</span>, <span class="dv">194</span>, <span class="dv">287</span>, <span class="dv">416</span>])</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Test fit_predict</span></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> FoolishClassifier(<span class="st">&#39;mode&#39;</span>, n_classes)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> clf.fit_predict(X, y)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>label, counts <span class="op">=</span> np.unique(predictions, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> label[<span class="dv">0</span>]<span class="op">==</span>clf.mode</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(counts)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.array_equal(counts, [<span class="dv">1000</span>])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>[267 248 257 228]
[1000]
[103 194 287 416]
[1000]
</code></pre>
</div>
</div>
<section id="testing-and-visualization" class="cell markdown"
id="aoEmTru62wNs">
<h2>Testing and visualization</h2>
<p>We'll create a synthetic data set, as we did last time, then you'll
train four models on the train split, and make predictions on the test
set split. Then we'll plot those predictions. The four models you'll
train and test will be:</p>
<ol>
<li>Gaussian Naive Bayes</li>
<li>FoolishClassifier with uniform distribution</li>
<li>FoolishClassifier with mode distribution</li>
<li>FoolishClassifier with proportional distribution</li>
</ol>
<p>The Gaussian Naive Bayes class, <a
href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html">GaussianNB</a>
was imported for you at that beginning of the notebook.</p>
</section>
<div class="cell code" id="oIPKzR9c2wNt">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create random samples of three classes, with samples drawn from multivariate Gaussian distributions</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">100</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>n1 <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>mean1 <span class="op">=</span> [<span class="fl">1.5</span>, <span class="dv">3</span>]</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>cov1 <span class="op">=</span> [[<span class="fl">0.5</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="fl">0.5</span>]]</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>n2 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>mean2 <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">2</span>]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>cov2 <span class="op">=</span> [[<span class="fl">0.1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>]]</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>n3 <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>mean3 <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>cov3 <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>]]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [np.random.multivariate_normal(mean1, cov1, size<span class="op">=</span>n1),</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>            np.random.multivariate_normal(mean2, cov2, size<span class="op">=</span>n2),</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>            np.random.multivariate_normal(mean3, cov3, size<span class="op">=</span>n3),</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>           ]</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> np.concatenate(features, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.array([<span class="dv">0</span>]<span class="op">*</span>n1 <span class="op">+</span> [<span class="dv">1</span>]<span class="op">*</span>n2 <span class="op">+</span> [<span class="dv">2</span>]<span class="op">*</span>n3)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(features, labels, random_state<span class="op">=</span><span class="dv">100</span>, test_size<span class="op">=</span><span class="fl">0.5</span>)</span></code></pre></div>
</div>
<div class="cell code" data-deletable="false" id="y24Gp22z2wNt"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;96b238e3a53a146bc7d5e64fa24a13af&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;Make_predictions&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># In this cell, instantiate the four models. Your FoolishClassifier</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># models should be instantiated with n_classes=3.</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Train your models using training features, X_train, and labels, y_train.</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Then, predict the test set labels using X_test. Save your predictions</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># to variables with the following names:</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">#    pred_nb, pred_uniform, pred_mode, pred_proportional</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>n_classes <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the models</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>nb <span class="op">=</span> GaussianNB()</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>uniform <span class="op">=</span> FoolishClassifier(<span class="st">&#39;uniform&#39;</span>, n_classes)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>mode <span class="op">=</span> FoolishClassifier(<span class="st">&#39;mode&#39;</span>, n_classes)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>proportional <span class="op">=</span> FoolishClassifier(<span class="st">&#39;proportional&#39;</span>, n_classes)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the models</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>nb.fit(X_train, y_train)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>uniform.fit(X_train, y_train)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>mode.fit(X_train, y_train)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>proportional.fit(X_train, y_train)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the test data</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>pred_nb <span class="op">=</span> nb.predict(X_test)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>pred_uniform <span class="op">=</span> uniform.predict(X_test)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>pred_mode <span class="op">=</span> mode.predict(X_test)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>pred_proportional <span class="op">=</span> proportional.predict(X_test)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="co"># raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell markdown" id="p90FHry92wNu">
<p>We'll plot your results for you in the cell below. Do they look
plausible to you, given what each model is doing? In the subsequent cell
we'll test your predictions with some <code>assert</code>
statements.</p>
</div>
<div class="cell code" id="XdVFFLqG2wNu">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test[:, <span class="dv">0</span>], X_test[:, <span class="dv">1</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, c<span class="op">=</span>y_test, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;True test data categories&#39;</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test[:, <span class="dv">0</span>], X_test[:, <span class="dv">1</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, c<span class="op">=</span>pred_nb, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Naive Bayes predictions&#39;</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test[:, <span class="dv">0</span>], X_test[:, <span class="dv">1</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, c<span class="op">=</span>pred_uniform, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Foolish-uniform predictions&#39;</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test[:, <span class="dv">0</span>], X_test[:, <span class="dv">1</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, c<span class="op">=</span>pred_mode, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Foolish-mode predictions&#39;</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">6</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_test[:, <span class="dv">0</span>], X_test[:, <span class="dv">1</span>], marker<span class="op">=</span><span class="st">&#39;o&#39;</span>, c<span class="op">=</span>pred_proportional, s<span class="op">=</span><span class="dv">50</span>, edgecolor<span class="op">=</span><span class="st">&#39;k&#39;</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> plt.title(<span class="st">&#39;Foolish-proportional predictions&#39;</span>)</span></code></pre></div>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
data-deletable="false" data-editable="false" id="p408WIld2wNv"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;957619ba5d3b9e4300114e8b8f64056b&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;Test_test_set_predictions&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}"
data-outputId="39c56571-5a91-4bb9-864d-507dd4ec3d11">
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Autograder: Test predictions of all classifiers, assuring</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># they match the expected results (which may or may not be</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the true classes/labels).</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>n_correct_nb <span class="op">=</span> np.<span class="bu">sum</span>(y_test<span class="op">==</span>pred_nb)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(n_correct_nb)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> n_correct_nb<span class="op">==</span><span class="dv">43</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>n_correct_uniform <span class="op">=</span> np.<span class="bu">sum</span>(y_test<span class="op">==</span>pred_uniform)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(n_correct_uniform)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> n_correct_uniform<span class="op">==</span><span class="dv">19</span> <span class="kw">or</span> n_correct_uniform<span class="op">==</span><span class="dv">14</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>n_correct_mode <span class="op">=</span> np.<span class="bu">sum</span>(y_test<span class="op">==</span>pred_mode)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(n_correct_mode)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> n_correct_mode<span class="op">==</span><span class="dv">40</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>n_correct_proportional <span class="op">=</span> np.<span class="bu">sum</span>(y_test<span class="op">==</span>pred_proportional)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(n_correct_proportional)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> n_correct_proportional<span class="op">==</span><span class="dv">35</span> <span class="kw">or</span> n_correct_proportional<span class="op">==</span><span class="dv">33</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>43
19
40
35
</code></pre>
</div>
</div>
<section id="evaluation" class="cell markdown" id="SEib3FWE2wNv">
<h2>Evaluation</h2>
<p>As in Part 1, we'll use the following to help us evaluate model
performance:</p>
<ul>
<li>Accuracy scores</li>
<li>Confusion matrices</li>
<li>Precision, Recall, and F-scores</li>
</ul>
<p>You have three more coding tasks... (1) implementing your own
function for computing accuracy, (2) implementing your own function for
computing a confusion matrix, and (3) using scikit-learn to compute
F-scores. You may not use scikit-learn for 1 and 2 (though you may want
to use the scikit-learn version during development, to make sure your
results match those of scikit-learn).</p>
<p>We'll start off with the accuracy function.</p>
</section>
<section id="accuracy-score" class="cell markdown" id="A0nxv4PR2wNv">
<h3>Accuracy score</h3>
</section>
<div class="cell code" data-deletable="false" id="1uYoocrT2wNw"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;4889909bc35cb5a95f1fc8130af0e48f&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;Accuracy_function&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Implement your accuracy function.</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># You may not use scikit-learn in your implementation.</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_accuracy_score(y_true, y_pred):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_true: An iterable of true label values.</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_pred: An iterable of predicted label values.</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Returns the fraction of predicted label values that match the true label values,</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># a number between 0 and 1.</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="bu">len</span>(y_true)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> true, pred <span class="kw">in</span> <span class="bu">zip</span>(y_true, y_pred):</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> true <span class="op">==</span> pred:</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> correct <span class="op">/</span> total</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell markdown" id="gclCjil52wNw">
<p>Let's see how accurate the models are, and test your code to make
sure the accuracy scores are correct.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="07ZxEDFc2wNw" data-outputId="6954ba30-34a6-4779-f03f-eaa24c2cc3a1">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>acc_nb <span class="op">=</span> my_accuracy_score(y_test, pred_nb)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>acc_uniform <span class="op">=</span> my_accuracy_score(y_test, pred_uniform)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>acc_mode <span class="op">=</span> my_accuracy_score(y_test, pred_mode)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>acc_proportional <span class="op">=</span> my_accuracy_score(y_test, pred_proportional)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Naive Bayes accuracy:          </span><span class="sc">{</span>acc_nb<span class="sc">:0.3f}</span><span class="ss">&#39;</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Foolish-uniform accuracy:      </span><span class="sc">{</span>acc_uniform<span class="sc">:0.3f}</span><span class="ss">&#39;</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Foolish-mode accuracy:         </span><span class="sc">{</span>acc_mode<span class="sc">:0.3f}</span><span class="ss">&#39;</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Foolish-proportional accuracy: </span><span class="sc">{</span>acc_proportional<span class="sc">:0.3f}</span><span class="ss">&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Naive Bayes accuracy:          0.860
Foolish-uniform accuracy:      0.380
Foolish-mode accuracy:         0.800
Foolish-proportional accuracy: 0.700
</code></pre>
</div>
</div>
<div class="cell code" data-deletable="false" data-editable="false"
id="Vnzqw_Jm2wNx"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;591d96de0338133a6656674fe57ad2f5&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;Test_accuracy_function&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Autograder: Confirm that accuracy scores were computed correctly</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(acc_nb, <span class="fl">0.86</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(acc_uniform, <span class="fl">0.38</span>) <span class="kw">or</span> np.isclose(acc_uniform, <span class="fl">0.28</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(acc_mode, <span class="fl">0.80</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(acc_proportional, <span class="fl">0.70</span>) <span class="kw">or</span> np.isclose(acc_proportional, <span class="fl">0.66</span>)</span></code></pre></div>
</div>
<div class="cell markdown" id="kIxh3B4r2wNx">
<p>Take a look at the accuracy scores, and at the scatter plots of
predictions (several cells above). Do they make sense?</p>
<p>The Foolish classifier is very foolish. It doesn't use the features
at all when making its predictions. <strong>Yet, the accuracy scores of
the 'mode' classifier and the 'proportional' classifier are almost as
high as that of the Naive Bayes model!</strong> Ask yourself why that
is, and if it isn't clear be sure to ask the instructional team about
it.</p>
<p>At this point it is hopefully evident that accuracy is sometimes a
poor metric for evaluating a model's performance.</p>
<p>Let's move on to your confusion matrix function.</p>
</div>
<section id="confusion-matrices" class="cell markdown"
id="CMkN_jq72wNx">
<h3>Confusion matrices</h3>
</section>
<div class="cell code" data-deletable="false" id="NawmWlxP2wNy"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;1110046a351bb144a32a8f3a5a4f8e17&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;ConfusionMat_function&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Implement your confusion matrix function.</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># You may not use scikit-learn in your implementation.</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_confusion_matrix(y_true, y_pred, n_classes):</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_true: An iterable of true label values.</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y_pred: An iterable of predicted label values.</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># n_classes: The number of classes. User-provided labels in y_true and</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#            y_pred must be values from 0 to n_classes-1.</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Returns a confusion matrix numpy array of shape (n_classes, n_classes).</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   For row i, column j, the confusion matrix contains the number of</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#   samples that had true label i, and predicted label j.</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Guidance:</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># There are many ways to do this, but here is one approach:</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Initialize a confusion matrix with all zeros, using numpy.zeros().</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Use a for loop to loop over the rows (true classes)</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Within that loop, use a for loop to loop over the columns (predicted classes)</span></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. Using the row and column loop counters, examine each sample in</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    y_true and y_pred and see if they are of the class for that row</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    and column. If so, add 1 to the confusion matrix at that row and column.</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    confusion <span class="op">=</span> np.zeros((n_classes, n_classes), dtype<span class="op">=</span>np.int32)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes):</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_classes):</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>            confusion[i, j] <span class="op">=</span> np.<span class="bu">sum</span>((y_true <span class="op">==</span> i) <span class="op">&amp;</span> (y_pred <span class="op">==</span> j))</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> confusion</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell markdown" id="Wd-ako7R2wNy">
<p>Let's look at the confusion matrices produced by your function, and
test your code to make sure they are correct.</p>
<p><strong>Recall that a good model will produce a confusion matrix that
has larger values along the diagonal and smaller (ideally 0) values off
the diagonal.</strong></p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="kqK3HDgj2wNy" data-outputId="6b310874-2eb7-40a9-ef73-df1664a01519">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>cm_nb <span class="op">=</span> my_confusion_matrix(y_test, pred_nb, n_classes)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>cm_uniform <span class="op">=</span> my_confusion_matrix(y_test, pred_uniform, n_classes)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>cm_mode <span class="op">=</span> my_confusion_matrix(y_test, pred_mode, n_classes)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>cm_proportional <span class="op">=</span> my_confusion_matrix(y_test, pred_proportional, n_classes)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Naive Bayes&#39;</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm_nb)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">Foolish-uniform&#39;</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm_uniform)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">Foolish-mode&#39;</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm_mode)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">Foolish-proportional&#39;</span>)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm_proportional)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Naive Bayes
[[38  2  0]
 [ 2  2  1]
 [ 2  0  3]]

Foolish-uniform
[[13  6 21]
 [ 0  4  1]
 [ 1  2  2]]

Foolish-mode
[[40  0  0]
 [ 5  0  0]
 [ 5  0  0]]

Foolish-proportional
[[34  6  0]
 [ 5  0  0]
 [ 4  0  1]]
</code></pre>
</div>
</div>
<div class="cell code" data-deletable="false" data-editable="false"
id="zr2YShR62wNz"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;380cdf74d785145c04ba2b3f373edc81&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;Test_ConfusionMat_function&quot;,&quot;locked&quot;:true,&quot;points&quot;:15,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Autograder: Confirm that confusion scores were computed correctly</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">int</span>(cm_nb[<span class="dv">0</span>, <span class="dv">0</span>])<span class="op">==</span><span class="dv">38</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">int</span>(cm_uniform[<span class="dv">2</span>, <span class="dv">2</span>])<span class="op">==</span><span class="dv">2</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">int</span>(cm_mode[<span class="dv">1</span>, <span class="dv">0</span>])<span class="op">==</span><span class="dv">5</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">int</span>(cm_proportional[<span class="dv">0</span>, <span class="dv">1</span>])<span class="op">==</span><span class="dv">6</span></span></code></pre></div>
</div>
<div class="cell markdown" id="86oHyVmZ2wNz">
<p>Review the confusion matrices. <strong>Can you see how they are more
informative than accuracy scores, and indicate what types of errors the
classifiers are making?</strong></p>
<p>The Foolish-mode confusion matrix has all zeros in columns 1 and 2,
making it clear that the classifier <strong>never</strong> predicts a
sample to be of class 1 and 2. Bad news! And while the
Foolish-proportional classifier sometimes predicts a sample to be of
class 1 and 2, those predictions are usually incorrect. Finally, we see
that the Foolish-uniform classifier is often predicting true samples of
class 0 to be of class 1 or 2 (the off-diagonal counts in the top
row).</p>
<p>Now that you've observed how informative a confusion matrix is, we'll
have you look at precision, recall, and F-score. You can use the
scikit-learn <a
href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html">precision_recall_fscore_support</a>
function rather than building your own.</p>
</div>
<section id="precision-recall-and-f-score" class="cell markdown"
id="Ju_Vnght2wNz">
<h3>Precision, Recall, and F-score</h3>
<p>You may want to review what precision, recall, and F-score are by
reading the header of the scikit-learn documentation (see link above).
In short...</p>
<ul>
<li><strong>Precision</strong> measures how often a sample predicted to
be of class i is truly of class i.</li>
<li><strong>Recall</strong> measures how often a sample that is
<strong>not</strong> of class i is correctly predicted to be
<strong>not</strong> of class i.</li>
<li><strong>F-score</strong> is the harmonic mean of precision and
recall: (2 x precision x recall) / (precision + recall). This means that
if either precision or recall are low, then f-score will be low.
<strong>F-score is a very effective metric for model performance on
multiclass classification tasks.</strong></li>
</ul>
<p>Note that precision, recall, and F-score are computed for each class,
so you'll get three values for each, for each classifier. <strong>To
prevent the analysis for getting to messy, we'll ask that you only save
the three f-scores for each classifier, and examine them.</strong> You
may of course save and print out the precision and recall scores as
well, for your own independent observation.</p>
<p>You will get an <code>UndefinedMetricWarning</code> which indicates
that a particular class/label was never predicted (e.g., by the
Foolish-mode classifier) and thus division by zero would occur in the
computations. You should read the warning, but its presence is expected
and of no concern.</p>
</section>
<div class="cell code" data-deletable="false" id="jgNuhGiw2wN0"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;63bf2283eaa81133619f8816a1a89a45&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;Compute_Fscores&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true,&quot;task&quot;:false}">
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use scikit-learn to compute the fscore for each of your classifiers.</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Save results to variables named: fscore_nb, fscore_uniform,</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># fscore_mode, and fscore_proportional.</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># YOUR CODE HERE</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># compute f1-score for each classifier</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>fscore_nb <span class="op">=</span> f1_score(y_test, pred_nb, labels<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>], average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>fscore_uniform <span class="op">=</span> f1_score(y_test, pred_uniform, labels<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>], average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>fscore_mode <span class="op">=</span> f1_score(y_test, pred_mode, labels<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>], average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>fscore_proportional <span class="op">=</span> f1_score(y_test, pred_proportional, labels<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>], average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co"># raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell markdown" id="a_hCbI132wN0">
<p>Finally, let's observe your F-scores, and make sure they are
correct.</p>
</div>
<div class="cell code"
data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}"
id="IVGJsufN2wN0" data-outputId="70d21142-ce7a-4e7c-a389-1eda5e12cbca">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Naive Bayes&#39;</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fscore_nb)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">Foolish-uniform&#39;</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fscore_uniform)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">Foolish-mode&#39;</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fscore_mode)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;</span><span class="ch">\n</span><span class="st">Foolish-proportional&#39;</span>)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fscore_proportional)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Naive Bayes
[0.92682927 0.44444444 0.66666667]

Foolish-uniform
[0.48148148 0.47058824 0.13793103]

Foolish-mode
[0.88888889 0.         0.        ]

Foolish-proportional
[0.81927711 0.         0.33333333]
</code></pre>
</div>
</div>
<div class="cell code" data-deletable="false" data-editable="false"
id="diR9EYRe2wN0"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;e929df926a7357549409bd62cbc2183b&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;Test_Fscores&quot;,&quot;locked&quot;:true,&quot;points&quot;:5,&quot;schema_version&quot;:3,&quot;solution&quot;:false,&quot;task&quot;:false}">
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Autograder: Test the F-scores for correctness</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(fscore_nb[<span class="dv">0</span>], <span class="fl">0.92682927</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(fscore_uniform[<span class="dv">1</span>], <span class="fl">0.47058824</span>) <span class="kw">or</span> np.isclose(fscore_uniform[<span class="dv">1</span>], <span class="fl">0.09090909</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(fscore_mode[<span class="dv">0</span>], <span class="fl">0.88888889</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> np.isclose(fscore_proportional[<span class="dv">2</span>], <span class="fl">0.33333333</span>) <span class="kw">or</span> np.isclose(fscore_proportional[<span class="dv">2</span>], <span class="fl">0.</span>)</span></code></pre></div>
</div>
<div class="cell markdown" id="8rBIhRGm2wN1">
<p><strong>Note that based on F-score, all three Foolish classifiers
performed much worse than the Naive Bayes classifier</strong>, when one
takes into consideration the individual scores for each class. If one
wanted to, taking the average F-score across all three classes would
provide a meaningfull <strong>scalar-valued</strong> metric of overall
model performance.</p>
</div>
<section id="youre-done-with-case-study-2" class="cell markdown"
id="lIKqP3rk2wN1">
<h2>You're done with Case Study 2!</h2>
<p><strong>Hopefully you now have a much better understanding of ML
models, including implementation, fitting/training, predicting, and ways
of evaluating performance (without being fooled by high scores on the
more simplistic evaluation metrics).</strong></p>
</section>
<section id="feedback" class="cell markdown" id="utUPJXH42wN1">
<h2>Feedback</h2>
</section>
<div class="cell code" data-deletable="false" id="TkcnYO4N2wN1"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;32a8c7a18d041ffd1b7e27ee6c95c484&quot;,&quot;grade&quot;:false,&quot;grade_id&quot;:&quot;Feedback&quot;,&quot;locked&quot;:false,&quot;schema_version&quot;:3,&quot;solution&quot;:true}">
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feedback():</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Provide feedback on the contents of this exercise</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co">        string</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># YOUR CODE HERE</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># N/A</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># raise NotImplementedError()</span></span></code></pre></div>
</div>
<div class="cell code" data-deletable="false" data-editable="false"
id="DVMaNyM_2wN2"
data-nbgrader="{&quot;cell_type&quot;:&quot;code&quot;,&quot;checksum&quot;:&quot;c34c64dab434b0766fa5e68ab5725830&quot;,&quot;grade&quot;:true,&quot;grade_id&quot;:&quot;Feedback_answer&quot;,&quot;locked&quot;:true,&quot;points&quot;:0,&quot;schema_version&quot;:3,&quot;solution&quot;:false}">
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"></code></pre></div>
</div>
</body>
</html>
